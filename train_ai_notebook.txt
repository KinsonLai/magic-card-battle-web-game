
# ==========================================
# 步驟 1: 安裝與引入套件
# ==========================================
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt
import io
import os

# 檢測環境 (Colab 或 本地)
try:
    from google.colab import files
    IS_COLAB = True
except ImportError:
    IS_COLAB = False

# 設定裝置 (優先使用 GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"使用裝置: {device}")

# ==========================================
# 步驟 2: 上傳訓練數據 (.json)
# 請執行此區塊後，點擊「選擇檔案」上傳 mcts_training_data_xxxx.json
# ==========================================
raw_data = []

if IS_COLAB:
    print("請上傳您的 mcts_training_data_xxxx.json 檔案...")
    uploaded = files.upload()
    if uploaded:
        filename = list(uploaded.keys())[0]
        print(f"已讀取檔案: {filename}")
        with open(filename, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
else:
    # 本地模式：自動搜尋目錄下的 json 檔案
    import glob
    json_files = glob.glob('mcts_training_data_*.json')
    if json_files:
        latest_file = max(json_files, key=os.path.getctime)
        print(f"讀取最新的數據檔案: {latest_file}")
        with open(latest_file, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
    else:
        print("錯誤：找不到 mcts_training_data_*.json 檔案，請確保檔案在同一目錄下。")

print(f"數據筆數: {len(raw_data)}")

# ==========================================
# 步驟 3: 數據預處理 (Data Preprocessing)
# ==========================================
class BattleDataset(Dataset):
    def __init__(self, data, scaler=None, nation_encoder=None):
        self.samples = []
        
        features = []
        nations = []
        value_targets = []
        
        for item in data:
            state_vec = []
            
            # 解析 Rich State JSON
            if 'state' in item:
                active_nation = item['player']
                players = item['state']['players']
                
                # 尋找當前視角玩家 (Active Player)
                me = next((p for p in players if p['nation'] == active_nation), None)
                
                if me:
                    # 提取關鍵特徵向量 (可依需求擴充)
                    # [HP, Mana, Gold, Soul, HandSize, LandsCount]
                    state_vec = [
                        float(me['hp']), 
                        float(me['mana']), 
                        float(me['gold']), 
                        float(me['soul']),
                        float(len(me['hand'])),
                        float(len(me['lands']))
                    ]
                else:
                    state_vec = [0.0] * 6
            
            # 兼容舊版數據
            elif 'stateVector' in item:
                state_vec = [float(x) for x in item['stateVector']]
            
            # 確保向量長度一致 (補 0)
            while len(state_vec) < 6:
                state_vec.append(0.0)
            
            features.append(state_vec[:6]) 
            nations.append([item['player']])
            value_targets.append(float(item['valueTarget']))

        features = np.array(features)
        
        # 數值標準化
        if scaler is None:
            self.scaler = StandardScaler()
            features_normalized = self.scaler.fit_transform(features)
        else:
            self.scaler = scaler
            features_normalized = self.scaler.transform(features)

        # 國家 One-Hot 編碼
        if nation_encoder is None:
            self.nation_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
            nations_encoded = self.nation_encoder.fit_transform(nations)
        else:
            self.nation_encoder = nation_encoder
            nations_encoded = self.nation_encoder.transform(nations)

        # 合併特徵
        self.X = np.hstack((features_normalized, nations_encoded))
        self.y_value = np.array(value_targets)
        
        # 轉為 PyTorch Tensor
        self.X = torch.FloatTensor(self.X)
        self.y_value = torch.FloatTensor(self.y_value).view(-1, 1)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y_value[idx]

# 建立 Dataset 與 DataLoader
if len(raw_data) > 0:
    full_dataset = BattleDataset(raw_data)
    input_dim = full_dataset.X.shape[1]

    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

    print(f"輸入特徵維度: {input_dim}")
    print(f"訓練集: {len(train_dataset)}, 驗證集: {len(val_dataset)}")
else:
    print("無有效數據，請檢查 JSON 檔案。")

# ==========================================
# 步驟 4: 定義模型 (ResNet-MLP)
# ==========================================
class ResidualBlock(nn.Module):
    def __init__(self, hidden_dim):
        super(ResidualBlock, self).__init__()
        self.fc = nn.Linear(hidden_dim, hidden_dim)
        self.bn = nn.BatchNorm1d(hidden_dim)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)

    def forward(self, x):
        residual = x
        out = self.fc(x)
        out = self.bn(out)
        out = self.relu(out)
        out = self.dropout(out)
        out += residual
        return out

class MagicZeroNet(nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super(MagicZeroNet, self).__init__()
        
        self.initial_layer = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU()
        )
        
        self.res_blocks = nn.Sequential(
            ResidualBlock(hidden_dim),
            ResidualBlock(hidden_dim),
            ResidualBlock(hidden_dim)
        )
        
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Tanh() # Output -1 to 1 (Win Rate)
        )

    def forward(self, x):
        x = self.initial_layer(x)
        x = self.res_blocks(x)
        value = self.value_head(x)
        return value

if 'input_dim' in locals():
    model = MagicZeroNet(input_dim=input_dim).to(device)
    print("模型已建立。")

# ==========================================
# 步驟 5: 開始訓練 (Training Loop)
# ==========================================
if 'train_loader' in locals() and len(raw_data) > 0:
    LEARNING_RATE = 0.001
    EPOCHS = 50
    patience = 10 

    criterion = nn.MSELoss()
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0

    print("--- 開始訓練 ---")

    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item() * inputs.size(0)
        
        epoch_train_loss = running_loss / len(train_dataset)
        train_losses.append(epoch_train_loss)

        model.eval()
        running_val_loss = 0.0
        with torch.no_grad():
            for inputs, targets in val_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, targets)
                running_val_loss += loss.item() * inputs.size(0)
                
        epoch_val_loss = running_val_loss / len(val_dataset)
        val_losses.append(epoch_val_loss)
        
        scheduler.step(epoch_val_loss)

        print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}")

        if epoch_val_loss < best_val_loss:
            best_val_loss = epoch_val_loss
            torch.save(model.state_dict(), 'best_model.pth')
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break

    print("訓練完成！")
    
    # 繪圖
    plt.figure(figsize=(10, 5))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.title('Training Process')
    plt.xlabel('Epochs')
    plt.ylabel('MSE Loss')
    plt.legend()
    plt.grid(True)
    plt.show()

# ==========================================
# 步驟 6: 測試預測 (Inference Test)
# ==========================================
if 'full_dataset' in locals() and os.path.exists('best_model.pth'):
    model.load_state_dict(torch.load('best_model.pth'))
    model.eval()

    def predict_win_rate(hp, mana, gold, soul, hand_size, lands_count, nation):
        # 構造輸入
        raw_input = np.array([[hp, mana, gold, soul, hand_size, lands_count]])
        
        # 標準化
        norm_input = full_dataset.scaler.transform(raw_input)
        
        # 國家編碼
        nation_input = full_dataset.nation_encoder.transform([[nation]])
        
        # 組合
        final_input = np.hstack((norm_input, nation_input))
        tensor_input = torch.FloatTensor(final_input).to(device)
        
        with torch.no_grad():
            score = model(tensor_input).item()
        
        return score

    print("\n--- AI 預測測試 ---")
    score1 = predict_win_rate(hp=200, mana=100, gold=500, soul=3, hand_size=8, lands_count=5, nation='MAGIC')
    print(f"情境 1 [優勢]: {score1:.3f} (預期 > 0.5)")

    score2 = predict_win_rate(hp=10, mana=0, gold=0, soul=-3, hand_size=0, lands_count=0, nation='FIGHTER')
    print(f"情境 2 [劣勢]: {score2:.3f} (預期 < -0.5)")

# ==========================================
# 步驟 7: 匯出 Weights 為 JSON (Web 使用)
# ==========================================
if 'model' in locals():
    state_dict = model.state_dict()
    weights_export = {}

    def tensor_to_list(tensor):
        return tensor.cpu().detach().numpy().tolist()

    # Manually map layers for JSON structure expected by frontend
    weights_export['initial_layer'] = {
        0: {'weight': tensor_to_list(state_dict['initial_layer.0.weight']), 'bias': tensor_to_list(state_dict['initial_layer.0.bias'])},
        1: {'weight': tensor_to_list(state_dict['initial_layer.1.weight']), 'bias': tensor_to_list(state_dict['initial_layer.1.bias'])}
    }
    
    weights_export['res_blocks'] = {}
    for i in range(3):
        weights_export['res_blocks'][i] = {
            'fc': {'weight': tensor_to_list(state_dict[f'res_blocks.{i}.fc.weight']), 'bias': tensor_to_list(state_dict[f'res_blocks.{i}.fc.bias'])},
            'bn': {'weight': tensor_to_list(state_dict[f'res_blocks.{i}.bn.weight']), 'bias': tensor_to_list(state_dict[f'res_blocks.{i}.bn.bias'])}
        }
        
    weights_export['value_head'] = {
        0: {'weight': tensor_to_list(state_dict['value_head.0.weight']), 'bias': tensor_to_list(state_dict['value_head.0.bias'])},
        2: {'weight': tensor_to_list(state_dict['value_head.2.weight']), 'bias': tensor_to_list(state_dict['value_head.2.bias'])}
    }

    with open('model_weights.json', 'w') as f:
        json.dump(weights_export, f)
        
    if IS_COLAB:
        print("下載 Web 用權重檔...")
        files.download('model_weights.json')
    else:
        print("Web 權重已儲存: model_weights.json")

# ==========================================
# 步驟 8: 下載原始模型 (備份)
# ==========================================
if 'full_dataset' in locals():
    # 儲存標準化參數
    scaler_params = {
        'mean': full_dataset.scaler.mean_.tolist(),
        'scale': full_dataset.scaler.scale_.tolist(),
        'nations': full_dataset.nation_encoder.categories_[0].tolist()
    }

    with open('model_params.json', 'w') as f:
        json.dump(scaler_params, f)

    if IS_COLAB:
        print("正在下載完整模型檔案...")
        files.download('best_model.pth')
        files.download('model_params.json')
    else:
        print("檔案已儲存於本地：best_model.pth, model_params.json")
